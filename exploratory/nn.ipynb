{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42be45c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with Wide, Deep, and Cross networks\n",
    "# https://keras.io/examples/structured_data/wide_deep_cross_networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "428e034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a3a69f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df_h = pd.read_csv('C:/Users/Simon/Documents/projects/MusicLabel/data/h_list.csv', index_col=0)\n",
    "df_s = pd.read_csv('C:/Users/Simon/Documents/projects/MusicLabel/data/s_list.csv', index_col=0)\n",
    "df = pd.concat([df_h, df_s], ignore_index=True)\n",
    "\n",
    "# Drop variables not used in model, and recode artist\n",
    "df.drop(['title', 'album', 'sr'], axis=1, inplace=True)\n",
    "artist_to_change = df.loc[df.groupby('artist')['artist'].transform('size') <= 2, 'artist']\n",
    "df.loc[df['artist'].isin(artist_to_change), 'artist'] = 'Other artists'\n",
    "df['artist'].fillna('Other artists', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9fa12495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split size: 250\n",
      "Test split size: 58\n"
     ]
    }
   ],
   "source": [
    "train_splits = []\n",
    "test_splits = []\n",
    "split = 0.2\n",
    "\n",
    "for _, group_data in df.groupby(\"label\"):\n",
    "    random_selection = np.random.rand(len(group_data.index)) <= (1-split)\n",
    "    train_splits.append(group_data[random_selection])\n",
    "    test_splits.append(group_data[~random_selection])\n",
    "\n",
    "train_data = pd.concat(train_splits).sample(frac=1).reset_index(drop=True)\n",
    "test_data = pd.concat(test_splits).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train split size: {len(train_data.index)}\")\n",
    "print(f\"Test split size: {len(test_data.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "769d28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = \"train_data.csv\"\n",
    "test_data_file = \"test_data.csv\"\n",
    "\n",
    "train_data.to_csv(train_data_file, index=False)\n",
    "test_data.to_csv(test_data_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "683b7510",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FEATURE_NAME = \"label\"\n",
    "TARGET_FEATURE_LABELS = [\"0\", \"1\"]\n",
    "NUM_CLASSES = len(TARGET_FEATURE_LABELS)\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = df.select_dtypes(include=np.number).columns.tolist()[1:] # exclude label\n",
    "\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"artist\": list(df[\"artist\"].unique())\n",
    "}\n",
    "\n",
    "CSV_HEADER = df.columns.tolist()\n",
    "\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + [TARGET_FEATURE_NAME] else [\"NA\"]\n",
    "    for feature_name in df.columns.tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9aaff162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_from_csv(csv_file_path, batch_size, shuffle=False):\n",
    "\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=True,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "018c4288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float64\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "beefb093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_inputs(inputs, use_embedding=False):\n",
    "    encoded_features = []\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "            # Create a lookup to convert string values to an integer indices.\n",
    "            # Since we are not using a mask token nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
    "            lookup = StringLookup(\n",
    "                vocabulary=vocabulary,\n",
    "                mask_token=None,\n",
    "                num_oov_indices=0,\n",
    "                output_mode=\"int\" if use_embedding else \"binary\",\n",
    "            )\n",
    "            if use_embedding:\n",
    "                # Convert the string input values into integer indices.\n",
    "                encoded_feature = lookup(inputs[feature_name])\n",
    "                embedding_dims = int(math.sqrt(len(vocabulary)))\n",
    "                # Create an embedding layer with the specified dimensions.\n",
    "                embedding = layers.Embedding(\n",
    "                    input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "                )\n",
    "                # Convert the index values to embedding representations.\n",
    "                encoded_feature = embedding(encoded_feature)\n",
    "            else:\n",
    "                # Convert the string input values into a one hot encoding.\n",
    "                encoded_feature = lookup(tf.expand_dims(inputs[feature_name], -1))\n",
    "        else:\n",
    "            # Use the numerical features as-is.\n",
    "            encoded_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "\n",
    "        encoded_features.append(encoded_feature)\n",
    "\n",
    "    all_features = layers.concatenate(encoded_features)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2c7a97b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_model():\n",
    "    inputs = create_model_inputs()\n",
    "    features = encode_inputs(inputs)\n",
    "\n",
    "    for units in hidden_units:\n",
    "        features = layers.Dense(units)(features)\n",
    "        features = layers.BatchNormalization()(features)\n",
    "        features = layers.ReLU()(features)\n",
    "        features = layers.Dropout(dropout_rate)(features)\n",
    "\n",
    "    outputs = layers.Dense(units=NUM_CLASSES, activation=\"softmax\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b1ae7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "    test_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(train_dataset, epochs=num_epochs)\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    _, accuracy = model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8dbb45aa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "dropout_rate = 0.1\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "hidden_units = [32, 32]\n",
    "\n",
    "baseline_model = create_baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20085aa9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 46s 60ms/step - loss: 0.0895 - sparse_categorical_accuracy: 0.9603\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.0973 - sparse_categorical_accuracy: 0.9762\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0869 - sparse_categorical_accuracy: 0.9643\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0904 - sparse_categorical_accuracy: 0.9643\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0786 - sparse_categorical_accuracy: 0.9762\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0619 - sparse_categorical_accuracy: 0.9683\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0366 - sparse_categorical_accuracy: 0.9802\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.0554 - sparse_categorical_accuracy: 0.9802\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.0444 - sparse_categorical_accuracy: 0.9921\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.0477 - sparse_categorical_accuracy: 0.9762\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0270 - sparse_categorical_accuracy: 0.9921\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.0553 - sparse_categorical_accuracy: 0.9881\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0258 - sparse_categorical_accuracy: 0.9921\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.0319 - sparse_categorical_accuracy: 0.9841\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0216 - sparse_categorical_accuracy: 0.9921\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0420 - sparse_categorical_accuracy: 0.9881\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0434 - sparse_categorical_accuracy: 0.9802\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0428 - sparse_categorical_accuracy: 0.9802\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0290 - sparse_categorical_accuracy: 0.9921\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.0305 - sparse_categorical_accuracy: 0.9921\n",
      "Model training finished\n",
      "Test accuracy: 60.71%\n"
     ]
    }
   ],
   "source": [
    "run_experiment(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1c2ce57",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 49s 64ms/step - loss: 0.7854 - sparse_categorical_accuracy: 0.6230\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.4730 - sparse_categorical_accuracy: 0.7540\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.3635 - sparse_categorical_accuracy: 0.8056\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.3043 - sparse_categorical_accuracy: 0.8571\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2577 - sparse_categorical_accuracy: 0.8690\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2266 - sparse_categorical_accuracy: 0.9127\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2043 - sparse_categorical_accuracy: 0.9405\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.1808 - sparse_categorical_accuracy: 0.9524\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1659 - sparse_categorical_accuracy: 0.9643\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.1511 - sparse_categorical_accuracy: 0.9762\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.1347 - sparse_categorical_accuracy: 0.9841\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.1275 - sparse_categorical_accuracy: 0.9841\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.1147 - sparse_categorical_accuracy: 0.9841\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.1093 - sparse_categorical_accuracy: 0.9881\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.1027 - sparse_categorical_accuracy: 0.9881\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 0.0953 - sparse_categorical_accuracy: 0.9881\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0879 - sparse_categorical_accuracy: 0.9881\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0813 - sparse_categorical_accuracy: 0.9921\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0770 - sparse_categorical_accuracy: 0.9881\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0735 - sparse_categorical_accuracy: 0.9960\n",
      "Model training finished\n",
      "Test accuracy: 71.43%\n"
     ]
    }
   ],
   "source": [
    "def create_wide_and_deep_model():\n",
    "\n",
    "    inputs = create_model_inputs()\n",
    "    wide = encode_inputs(inputs)\n",
    "    wide = layers.BatchNormalization()(wide)\n",
    "\n",
    "    deep = encode_inputs(inputs, use_embedding=True)\n",
    "    for units in hidden_units:\n",
    "        deep = layers.Dense(units)(deep)\n",
    "        deep = layers.BatchNormalization()(deep)\n",
    "        deep = layers.ReLU()(deep)\n",
    "        deep = layers.Dropout(dropout_rate)(deep)\n",
    "\n",
    "    merged = layers.concatenate([wide, deep])\n",
    "    outputs = layers.Dense(units=NUM_CLASSES, activation=\"softmax\")(merged)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "wide_and_deep_model = create_wide_and_deep_model()\n",
    "run_experiment(wide_and_deep_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55f71d68",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 64s 68ms/step - loss: 0.1212 - sparse_categorical_accuracy: 0.9643\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0782 - sparse_categorical_accuracy: 0.9802\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0701 - sparse_categorical_accuracy: 0.9762\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0594 - sparse_categorical_accuracy: 0.9841\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0554 - sparse_categorical_accuracy: 0.9881\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0499 - sparse_categorical_accuracy: 0.9921\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0438 - sparse_categorical_accuracy: 0.9960\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0415 - sparse_categorical_accuracy: 0.9960\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0387 - sparse_categorical_accuracy: 0.9960\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0356 - sparse_categorical_accuracy: 1.0000\n",
      "Model training finished\n",
      "Test accuracy: 73.21%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "run_experiment(wide_and_deep_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "955a01e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/5\n",
      "8/8 [==============================] - 50s 69ms/step - loss: 0.0698 - sparse_categorical_accuracy: 0.9722\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.0396 - sparse_categorical_accuracy: 0.9841\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.0377 - sparse_categorical_accuracy: 0.9881\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0303 - sparse_categorical_accuracy: 0.9881\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.0299 - sparse_categorical_accuracy: 0.9921\n",
      "Model training finished\n",
      "Test accuracy: 66.07%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "run_experiment(wide_and_deep_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dc02cb0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 37s 120ms/step - loss: 1.2012 - sparse_categorical_accuracy: 0.5675\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.5152 - sparse_categorical_accuracy: 0.7460\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.4588 - sparse_categorical_accuracy: 0.7897\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 1s 102ms/step - loss: 0.2823 - sparse_categorical_accuracy: 0.8770\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.2287 - sparse_categorical_accuracy: 0.9008\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.1702 - sparse_categorical_accuracy: 0.9563\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.1422 - sparse_categorical_accuracy: 0.9563\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.1042 - sparse_categorical_accuracy: 0.9881\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 1s 102ms/step - loss: 0.0877 - sparse_categorical_accuracy: 0.9960\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.0721 - sparse_categorical_accuracy: 0.9960\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.0599 - sparse_categorical_accuracy: 0.9960\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 1s 105ms/step - loss: 0.0550 - sparse_categorical_accuracy: 0.9960\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.0472 - sparse_categorical_accuracy: 0.9960\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 1s 102ms/step - loss: 0.0444 - sparse_categorical_accuracy: 0.9921\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.0405 - sparse_categorical_accuracy: 0.9921\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.0415 - sparse_categorical_accuracy: 0.9921\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.0332 - sparse_categorical_accuracy: 0.9921\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 1s 102ms/step - loss: 0.0282 - sparse_categorical_accuracy: 0.9960\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.0214 - sparse_categorical_accuracy: 0.9960\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 1s 102ms/step - loss: 0.0199 - sparse_categorical_accuracy: 0.9960\n",
      "Model training finished\n",
      "Test accuracy: 62.5%\n"
     ]
    }
   ],
   "source": [
    "def create_deep_and_cross_model():\n",
    "\n",
    "    inputs = create_model_inputs()\n",
    "    x0 = encode_inputs(inputs, use_embedding=True)\n",
    "\n",
    "    cross = x0\n",
    "    for _ in hidden_units:\n",
    "        units = cross.shape[-1]\n",
    "        x = layers.Dense(units)(cross)\n",
    "        cross = x0 * x + cross\n",
    "    cross = layers.BatchNormalization()(cross)\n",
    "\n",
    "    deep = x0\n",
    "    for units in hidden_units:\n",
    "        deep = layers.Dense(units)(deep)\n",
    "        deep = layers.BatchNormalization()(deep)\n",
    "        deep = layers.ReLU()(deep)\n",
    "        deep = layers.Dropout(dropout_rate)(deep)\n",
    "\n",
    "    merged = layers.concatenate([cross, deep])\n",
    "    outputs = layers.Dense(units=NUM_CLASSES, activation=\"softmax\")(merged)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "deep_and_cross_model = create_deep_and_cross_model()\n",
    "\n",
    "\"\"\"\n",
    "Let's run it:\n",
    "\"\"\"\n",
    "\n",
    "run_experiment(deep_and_cross_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f92e2ee2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/9\n",
      "8/8 [==============================] - 40s 123ms/step - loss: 0.2333 - sparse_categorical_accuracy: 0.9365\n",
      "Epoch 2/9\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.1549 - sparse_categorical_accuracy: 0.9444\n",
      "Epoch 3/9\n",
      "8/8 [==============================] - 1s 100ms/step - loss: 0.1428 - sparse_categorical_accuracy: 0.9524\n",
      "Epoch 4/9\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.1075 - sparse_categorical_accuracy: 0.9563\n",
      "Epoch 5/9\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.0714 - sparse_categorical_accuracy: 0.9841\n",
      "Epoch 6/9\n",
      "8/8 [==============================] - 1s 100ms/step - loss: 0.0418 - sparse_categorical_accuracy: 0.9841\n",
      "Epoch 7/9\n",
      "8/8 [==============================] - 1s 100ms/step - loss: 0.0274 - sparse_categorical_accuracy: 0.9921\n",
      "Epoch 8/9\n",
      "8/8 [==============================] - 1s 100ms/step - loss: 0.0189 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 9/9\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 0.0150 - sparse_categorical_accuracy: 1.0000\n",
      "Model training finished\n",
      "Test accuracy: 64.29%\n"
     ]
    }
   ],
   "source": [
    "run_experiment(deep_and_cross_model, num_epochs=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2aca12e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvement over basic model, wide and deep seem to work best, but still cannot get me in the 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e41022be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with PCA using these NN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0feeb387-e28e-44be-b493-bbe6327378e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data.loc[:, ~train_data.columns.isin(['label'])]\n",
    "y_train = train_data.loc[:, 'label'].values\n",
    "\n",
    "x_test = test_data.loc[:, ~test_data.columns.isin(['label'])]\n",
    "y_test = test_data.loc[:, 'label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee71fad2-2d12-4b45-ac5c-551162c60206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA and use 95% variance explained\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), make_column_selector(dtype_exclude='object')),\n",
    "    ('cat', OneHotEncoder(), make_column_selector(dtype_include='object'))])\n",
    "x_train_pp = preprocessor.fit_transform(x_train)\n",
    "\n",
    "pca = PCA(.95)\n",
    "x_train_pca = pca.fit_transform(x_train_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44817e34-970c-4ee9-88aa-73a48ed8a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pp = preprocessor.transform(x_test)\n",
    "x_test_pca = pca.transform(x_test_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "773dd327-a7cd-4e77-8488-e0ebe8e51f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine back to a pd df to repeat the NN analysis above\n",
    "train_pca = pd.concat([pd.DataFrame(x_train_pca).add_prefix('pca_'), pd.DataFrame(y_train, columns=['label'])], axis=1)\n",
    "test_pca = pd.concat([pd.DataFrame(x_test_pca).add_prefix('pca_'), pd.DataFrame(y_test, columns=['label'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0870284d-5d05-4cd5-9ff9-ee808a8f4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FEATURE_NAME = \"label\"\n",
    "TARGET_FEATURE_LABELS = [\"0\", \"1\"]\n",
    "NUM_CLASSES = len(TARGET_FEATURE_LABELS)\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = train_pca.select_dtypes(include=np.number).columns.tolist()[:-1] # exclude label\n",
    "\n",
    "CSV_HEADER = train_pca.columns.tolist()\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES\n",
    "\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + [TARGET_FEATURE_NAME] else [\"NA\"]\n",
    "    for feature_name in train_pca.columns.tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4242d08b-de93-4877-b37d-22268e5f42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = \"train_data_pca.csv\"\n",
    "test_data_file = \"test_data_pca.csv\"\n",
    "\n",
    "train_pca.to_csv(train_data_file, index=False)\n",
    "test_pca.to_csv(test_data_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1537bc35-eb1b-443c-be9d-cfd65d231955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float64\n",
    "        )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c9cfcb63-90f8-426f-9b38-fd509556bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_inputs(inputs, use_embedding=False):\n",
    "    encoded_features = []\n",
    "    for feature_name in inputs:\n",
    "        encoded_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "        encoded_features.append(encoded_feature)\n",
    "\n",
    "    all_features = layers.concatenate(encoded_features)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a8e80063-4a2a-461f-855b-c4ffc7aa8879",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 4s 8ms/step - loss: 0.8079 - sparse_categorical_accuracy: 0.5401\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.8028 - sparse_categorical_accuracy: 0.5992\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7055 - sparse_categorical_accuracy: 0.6287\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6981 - sparse_categorical_accuracy: 0.6076\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6674 - sparse_categorical_accuracy: 0.6498\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6291 - sparse_categorical_accuracy: 0.6540\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7103 - sparse_categorical_accuracy: 0.6329\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6102 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6203 - sparse_categorical_accuracy: 0.6329\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6330 - sparse_categorical_accuracy: 0.6709\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6297 - sparse_categorical_accuracy: 0.6624\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.5244 - sparse_categorical_accuracy: 0.718 - 0s 6ms/step - loss: 0.6241 - sparse_categorical_accuracy: 0.6414\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6048 - sparse_categorical_accuracy: 0.6709\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6214 - sparse_categorical_accuracy: 0.6118\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6310 - sparse_categorical_accuracy: 0.6793\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6059 - sparse_categorical_accuracy: 0.6920\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6021 - sparse_categorical_accuracy: 0.6793\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6386 - sparse_categorical_accuracy: 0.6287\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5896 - sparse_categorical_accuracy: 0.6920\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5947 - sparse_categorical_accuracy: 0.7215\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5888 - sparse_categorical_accuracy: 0.6582\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5396 - sparse_categorical_accuracy: 0.7342\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5778 - sparse_categorical_accuracy: 0.7131\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5776 - sparse_categorical_accuracy: 0.7131\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5400 - sparse_categorical_accuracy: 0.7342\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5964 - sparse_categorical_accuracy: 0.7300\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5581 - sparse_categorical_accuracy: 0.7215\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5574 - sparse_categorical_accuracy: 0.6920\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5475 - sparse_categorical_accuracy: 0.7215\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5555 - sparse_categorical_accuracy: 0.7131\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5157 - sparse_categorical_accuracy: 0.7764\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5161 - sparse_categorical_accuracy: 0.7511\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5723 - sparse_categorical_accuracy: 0.6962\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5607 - sparse_categorical_accuracy: 0.6920\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5636 - sparse_categorical_accuracy: 0.6878\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5539 - sparse_categorical_accuracy: 0.7384\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5044 - sparse_categorical_accuracy: 0.7426\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5392 - sparse_categorical_accuracy: 0.7553\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5023 - sparse_categorical_accuracy: 0.7722\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5336 - sparse_categorical_accuracy: 0.7595\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5135 - sparse_categorical_accuracy: 0.7173\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4957 - sparse_categorical_accuracy: 0.7848\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5157 - sparse_categorical_accuracy: 0.7468\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4958 - sparse_categorical_accuracy: 0.7511\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4912 - sparse_categorical_accuracy: 0.7806\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4720 - sparse_categorical_accuracy: 0.7637\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4725 - sparse_categorical_accuracy: 0.7890\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4897 - sparse_categorical_accuracy: 0.7553\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4703 - sparse_categorical_accuracy: 0.7764\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4749 - sparse_categorical_accuracy: 0.7511\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4615 - sparse_categorical_accuracy: 0.7890\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4883 - sparse_categorical_accuracy: 0.7595\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4622 - sparse_categorical_accuracy: 0.7722\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4410 - sparse_categorical_accuracy: 0.7932\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4860 - sparse_categorical_accuracy: 0.7426\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4089 - sparse_categorical_accuracy: 0.8186\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4227 - sparse_categorical_accuracy: 0.8059\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4310 - sparse_categorical_accuracy: 0.8017\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4324 - sparse_categorical_accuracy: 0.8186\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4393 - sparse_categorical_accuracy: 0.7975\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4302 - sparse_categorical_accuracy: 0.8186\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4479 - sparse_categorical_accuracy: 0.8059\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3799 - sparse_categorical_accuracy: 0.8523\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4169 - sparse_categorical_accuracy: 0.8143\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3733 - sparse_categorical_accuracy: 0.8439\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3962 - sparse_categorical_accuracy: 0.8228\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3981 - sparse_categorical_accuracy: 0.8186\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4101 - sparse_categorical_accuracy: 0.8059\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3809 - sparse_categorical_accuracy: 0.8312\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4023 - sparse_categorical_accuracy: 0.8017\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3860 - sparse_categorical_accuracy: 0.8439\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3759 - sparse_categorical_accuracy: 0.8650\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3841 - sparse_categorical_accuracy: 0.8312\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3520 - sparse_categorical_accuracy: 0.8059\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3644 - sparse_categorical_accuracy: 0.8608\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3625 - sparse_categorical_accuracy: 0.8312\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4155 - sparse_categorical_accuracy: 0.8312\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3805 - sparse_categorical_accuracy: 0.8270\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3461 - sparse_categorical_accuracy: 0.8397\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3485 - sparse_categorical_accuracy: 0.8650\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3158 - sparse_categorical_accuracy: 0.8819\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3261 - sparse_categorical_accuracy: 0.8734\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3051 - sparse_categorical_accuracy: 0.8903\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3573 - sparse_categorical_accuracy: 0.8354\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3246 - sparse_categorical_accuracy: 0.8650\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3632 - sparse_categorical_accuracy: 0.8186\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3381 - sparse_categorical_accuracy: 0.8565\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2780 - sparse_categorical_accuracy: 0.8945\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3189 - sparse_categorical_accuracy: 0.8650\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3028 - sparse_categorical_accuracy: 0.8650\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3394 - sparse_categorical_accuracy: 0.8481\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2941 - sparse_categorical_accuracy: 0.8903\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2733 - sparse_categorical_accuracy: 0.9072\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2936 - sparse_categorical_accuracy: 0.8945\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2772 - sparse_categorical_accuracy: 0.8987\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3164 - sparse_categorical_accuracy: 0.8650\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2891 - sparse_categorical_accuracy: 0.9030\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3142 - sparse_categorical_accuracy: 0.8734\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3129 - sparse_categorical_accuracy: 0.8650\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2898 - sparse_categorical_accuracy: 0.8819\n",
      "Model training finished\n",
      "Test accuracy: 69.01%\n"
     ]
    }
   ],
   "source": [
    "hidden_units = [8, 8, 8, 8, 8]\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.1\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "baseline_model = create_baseline_model()\n",
    "run_experiment(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0056c468-656a-4cde-b0ce-6d325668d606",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_wide_and_deep_model():\n",
    "\n",
    "    inputs = create_model_inputs()\n",
    "    wide = encode_inputs(inputs)\n",
    "    wide = layers.BatchNormalization()(wide)\n",
    "\n",
    "    deep = encode_inputs(inputs, use_embedding=True)\n",
    "    for units in hidden_units:\n",
    "        deep = layers.Dense(units)(deep)\n",
    "        deep = layers.BatchNormalization()(deep)\n",
    "        deep = layers.ReLU()(deep)\n",
    "        deep = layers.Dropout(dropout_rate)(deep)\n",
    "\n",
    "    merged = layers.concatenate([wide, deep])\n",
    "    outputs = layers.Dense(units=NUM_CLASSES, activation=\"softmax\")(merged)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f83d00a6-3a10-4817-b29a-efeaf73ad409",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/30\n",
      "8/8 [==============================] - 5s 8ms/step - loss: 1.1095 - sparse_categorical_accuracy: 0.4557\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.0672 - sparse_categorical_accuracy: 0.4810\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.0079 - sparse_categorical_accuracy: 0.5274\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.9248 - sparse_categorical_accuracy: 0.5443\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.9166 - sparse_categorical_accuracy: 0.5443\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.8548 - sparse_categorical_accuracy: 0.5781\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7968 - sparse_categorical_accuracy: 0.6245\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7728 - sparse_categorical_accuracy: 0.6076\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7231 - sparse_categorical_accuracy: 0.6456\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6748 - sparse_categorical_accuracy: 0.6414\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6765 - sparse_categorical_accuracy: 0.6540\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6183 - sparse_categorical_accuracy: 0.7046\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5948 - sparse_categorical_accuracy: 0.6878\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5614 - sparse_categorical_accuracy: 0.7468\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5099 - sparse_categorical_accuracy: 0.7468\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5032 - sparse_categorical_accuracy: 0.7722\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4983 - sparse_categorical_accuracy: 0.7553\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4507 - sparse_categorical_accuracy: 0.7975\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4231 - sparse_categorical_accuracy: 0.7975\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3998 - sparse_categorical_accuracy: 0.8397\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3877 - sparse_categorical_accuracy: 0.8439\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3731 - sparse_categorical_accuracy: 0.8186\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3520 - sparse_categorical_accuracy: 0.8650\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3332 - sparse_categorical_accuracy: 0.8523\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3048 - sparse_categorical_accuracy: 0.9156\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3022 - sparse_categorical_accuracy: 0.8692\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2809 - sparse_categorical_accuracy: 0.9030\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2874 - sparse_categorical_accuracy: 0.8987\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3044 - sparse_categorical_accuracy: 0.8987\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2531 - sparse_categorical_accuracy: 0.9156\n",
      "Model training finished\n",
      "Test accuracy: 69.01%\n"
     ]
    }
   ],
   "source": [
    "hidden_units = [16, 16, 16]\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.1\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "wide_and_deep_model = create_wide_and_deep_model()\n",
    "run_experiment(wide_and_deep_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7466dff8-af15-4205-bcb7-ed4a61566e31",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_deep_and_cross_model():\n",
    "\n",
    "    inputs = create_model_inputs()\n",
    "    x0 = encode_inputs(inputs, use_embedding=True)\n",
    "\n",
    "    cross = x0\n",
    "    for _ in hidden_units:\n",
    "        units = cross.shape[-1]\n",
    "        x = layers.Dense(units)(cross)\n",
    "        cross = x0 * x + cross\n",
    "    cross = layers.BatchNormalization()(cross)\n",
    "\n",
    "    deep = x0\n",
    "    for units in hidden_units:\n",
    "        deep = layers.Dense(units)(deep)\n",
    "        deep = layers.BatchNormalization()(deep)\n",
    "        deep = layers.ReLU()(deep)\n",
    "        deep = layers.Dropout(dropout_rate)(deep)\n",
    "\n",
    "    merged = layers.concatenate([cross, deep])\n",
    "    outputs = layers.Dense(units=NUM_CLASSES, activation=\"softmax\")(merged)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d6d36e51-08f9-48d8-8c61-8cf5be56ec5c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/5\n",
      "8/8 [==============================] - 4s 8ms/step - loss: 0.9687 - sparse_categorical_accuracy: 0.4895\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4663 - sparse_categorical_accuracy: 0.7975\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3165 - sparse_categorical_accuracy: 0.9030\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2421 - sparse_categorical_accuracy: 0.9451\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1899 - sparse_categorical_accuracy: 0.9789\n",
      "Model training finished\n",
      "Test accuracy: 56.34%\n"
     ]
    }
   ],
   "source": [
    "hidden_units = [8, 8, 8]\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.1\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "\n",
    "deep_and_cross_model = create_deep_and_cross_model()\n",
    "run_experiment(deep_and_cross_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d73c5ba5-65d6-41ad-8326-eb9eecf14172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweaking these may improve it but doesn't seem likely to have a significant jump\n",
    "# PCA w/ NN didn't really improve the performance much like in RF/xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6088105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try alternative approach - instead of structured data, time series to perserve/represent music better\n",
    "# Another is a bit like time series but instead of time series data, use the sepctogram image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
