{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2c2f9c-3608-46d6-8295-e22d82d2b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/timeseries/timeseries_classification_from_scratch/\n",
    "# https://keras.io/examples/timeseries/timeseries_classification_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b3a5950-7235-4ef4-9283-3d10fe9b00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b15add4-60fa-4f14-974e-6183c570057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = pd.read_json('C:/Users/Simon/Documents/projects/MusicLabel/data/h_time_series.json')\n",
    "s = pd.read_json('C:/Users/Simon/Documents/projects/MusicLabel/data/s_time_series.json')\n",
    "df_raw = pd.concat([h, s], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a8ba8c-2d8d-4820-a720-382ae7a5a47d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 308 entries, 0 to 307\n",
      "Data columns (total 40 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   mfcc_0      308 non-null    object\n",
      " 1   mfcc_1      308 non-null    object\n",
      " 2   mfcc_2      308 non-null    object\n",
      " 3   mfcc_3      308 non-null    object\n",
      " 4   mfcc_4      308 non-null    object\n",
      " 5   mfcc_5      308 non-null    object\n",
      " 6   mfcc_6      308 non-null    object\n",
      " 7   mfcc_7      308 non-null    object\n",
      " 8   mfcc_8      308 non-null    object\n",
      " 9   mfcc_9      308 non-null    object\n",
      " 10  mfcc_10     308 non-null    object\n",
      " 11  mfcc_11     308 non-null    object\n",
      " 12  mfcc_12     308 non-null    object\n",
      " 13  mfcc_d1_0   308 non-null    object\n",
      " 14  mfcc_d1_1   308 non-null    object\n",
      " 15  mfcc_d1_2   308 non-null    object\n",
      " 16  mfcc_d1_3   308 non-null    object\n",
      " 17  mfcc_d1_4   308 non-null    object\n",
      " 18  mfcc_d1_5   308 non-null    object\n",
      " 19  mfcc_d1_6   308 non-null    object\n",
      " 20  mfcc_d1_7   308 non-null    object\n",
      " 21  mfcc_d1_8   308 non-null    object\n",
      " 22  mfcc_d1_9   308 non-null    object\n",
      " 23  mfcc_d1_10  308 non-null    object\n",
      " 24  mfcc_d1_11  308 non-null    object\n",
      " 25  mfcc_d1_12  308 non-null    object\n",
      " 26  mfcc_d2_0   308 non-null    object\n",
      " 27  mfcc_d2_1   308 non-null    object\n",
      " 28  mfcc_d2_2   308 non-null    object\n",
      " 29  mfcc_d2_3   308 non-null    object\n",
      " 30  mfcc_d2_4   308 non-null    object\n",
      " 31  mfcc_d2_5   308 non-null    object\n",
      " 32  mfcc_d2_6   308 non-null    object\n",
      " 33  mfcc_d2_7   308 non-null    object\n",
      " 34  mfcc_d2_8   308 non-null    object\n",
      " 35  mfcc_d2_9   308 non-null    object\n",
      " 36  mfcc_d2_10  308 non-null    object\n",
      " 37  mfcc_d2_11  308 non-null    object\n",
      " 38  mfcc_d2_12  308 non-null    object\n",
      " 39  label       308 non-null    int64 \n",
      "dtypes: int64(1), object(39)\n",
      "memory usage: 96.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# turn list of list into multiple time series list\n",
    "# so that each mfcc/d1/d2 (13*3) is a column\n",
    "mfcc = df_raw['mfcc']\n",
    "mfcc_d1 = df_raw['mfcc_d1']\n",
    "mfcc_d2 = df_raw['mfcc_d2']\n",
    "\n",
    "df = pd.concat([pd.DataFrame.from_records(mfcc, columns=[\"mfcc_\" + str(i) for i in range(13)]),\n",
    "               pd.DataFrame.from_records(mfcc_d1, columns=[\"mfcc_d1_\" + str(i) for i in range(13)]),\n",
    "               pd.DataFrame.from_records(mfcc_d2, columns=[\"mfcc_d2_\" + str(i) for i in range(13)]),\n",
    "               df_raw['label']], axis=1)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fb8ccf4-39e9-4f6f-aa7b-02181ed77fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum length of music\n",
    "# need to make it the same length\n",
    "# experiment with cutting from the middle or from the start\n",
    "# if worried about short songs gets to finish vs long - can also excerpt first minute or so\n",
    "min_length = df.loc[:, 'mfcc_0'].str.len().min()\n",
    "\n",
    "# separate out the features and labels - feature is to be cut\n",
    "df_x = df.loc[:, df.columns != 'label']\n",
    "df_y = df.loc[:, df.columns == 'label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "776b3871-63dd-42f0-8853-cf110fc5f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_from_middle(x, min_length):\n",
    "    midp = round(len(x)/2)\n",
    "    startp = round(midp - (min_length/2))\n",
    "    endp = round(midp + (min_length/2))\n",
    "    \n",
    "    return x[startp:endp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0ff990fa-fbda-455a-b02d-80543ab68ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cut the music from the middle\n",
    "df_mid_x = df_x.applymap(lambda x: cut_from_middle(x, min_length))\n",
    "\n",
    "# Verify the length - number of records not meeting the length\n",
    "df_mid_x[df_mid_x['mfcc_0'].str.len() != min_length].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e60d8331-37fa-4a4e-a02b-d3b134bf5256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 8512, 39)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the data into (nobs, each time series feature, n feature)\n",
    "l = []\n",
    "for col in df_mid_x.columns:\n",
    "    t = df_mid_x.loc[:, col].to_numpy()\n",
    "    l.append(np.concatenate(t).reshape(df_mid_x.shape[0], len(df_mid_x.iloc[0, 0])))\n",
    "    \n",
    "np_mid_x = np.dstack(l)\n",
    "np_mid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "61f9f675-bc53-46ed-aa4c-5c1cedc83af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train/test\n",
    "x_mid_train, x_mid_test, y_train, y_test = train_test_split(np_mid_x, df_y, test_size=0.2, random_state=363)\n",
    "\n",
    "# random sort the data for validation\n",
    "idx = np.random.permutation(len(x_mid_train))\n",
    "x_mid_train = x_mid_train[idx]\n",
    "y_train = y_train[idx]\n",
    "\n",
    "# define number of labels\n",
    "num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9134ba83-2f63-44dc-b446-3b62dc96e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "def make_model(input_shape):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "\n",
    "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=x_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "26163022-b9d0-4402-8d37-307f1eb1e225",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.2703 - sparse_categorical_accuracy: 0.9031 - val_loss: 0.3454 - val_sparse_categorical_accuracy: 0.8000\n",
      "INFO:tensorflow:Assets written to: cut_from_mid_conv\\assets\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.2120 - sparse_categorical_accuracy: 0.9235 - val_loss: 0.6620 - val_sparse_categorical_accuracy: 0.6800\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.2394 - sparse_categorical_accuracy: 0.9031 - val_loss: 0.3197 - val_sparse_categorical_accuracy: 0.8200\n",
      "INFO:tensorflow:Assets written to: cut_from_mid_conv\\assets\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 11s 2s/step - loss: 0.2215 - sparse_categorical_accuracy: 0.9133 - val_loss: 0.3902 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.2086 - sparse_categorical_accuracy: 0.9133 - val_loss: 0.3521 - val_sparse_categorical_accuracy: 0.8800\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1827 - sparse_categorical_accuracy: 0.9184 - val_loss: 0.2564 - val_sparse_categorical_accuracy: 0.9000\n",
      "INFO:tensorflow:Assets written to: cut_from_mid_conv\\assets\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.2110 - sparse_categorical_accuracy: 0.9388 - val_loss: 0.2900 - val_sparse_categorical_accuracy: 0.8600\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1840 - sparse_categorical_accuracy: 0.9337 - val_loss: 0.2553 - val_sparse_categorical_accuracy: 0.8800\n",
      "INFO:tensorflow:Assets written to: cut_from_mid_conv\\assets\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1815 - sparse_categorical_accuracy: 0.9388 - val_loss: 0.4170 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1818 - sparse_categorical_accuracy: 0.9388 - val_loss: 0.5047 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.2293 - sparse_categorical_accuracy: 0.8980 - val_loss: 0.6204 - val_sparse_categorical_accuracy: 0.7200\n",
      "Epoch 12/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1820 - sparse_categorical_accuracy: 0.9184 - val_loss: 0.5206 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1618 - sparse_categorical_accuracy: 0.9439 - val_loss: 0.3856 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1367 - sparse_categorical_accuracy: 0.9592 - val_loss: 0.3170 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1682 - sparse_categorical_accuracy: 0.9490 - val_loss: 0.7887 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.2526 - sparse_categorical_accuracy: 0.9133 - val_loss: 0.9964 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1672 - sparse_categorical_accuracy: 0.9337 - val_loss: 0.4195 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1796 - sparse_categorical_accuracy: 0.9337 - val_loss: 0.6325 - val_sparse_categorical_accuracy: 0.7400\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1764 - sparse_categorical_accuracy: 0.9439 - val_loss: 0.5758 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1297 - sparse_categorical_accuracy: 0.9643 - val_loss: 0.5399 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1480 - sparse_categorical_accuracy: 0.9388 - val_loss: 0.3773 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 22/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1302 - sparse_categorical_accuracy: 0.9541 - val_loss: 0.4067 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 23/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1136 - sparse_categorical_accuracy: 0.9643 - val_loss: 0.6362 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 24/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1485 - sparse_categorical_accuracy: 0.9490 - val_loss: 0.6946 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 25/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1644 - sparse_categorical_accuracy: 0.9439 - val_loss: 0.6509 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 26/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1705 - sparse_categorical_accuracy: 0.9286 - val_loss: 0.4973 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 27/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1820 - sparse_categorical_accuracy: 0.9235 - val_loss: 0.7012 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 28/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1468 - sparse_categorical_accuracy: 0.9286 - val_loss: 0.6779 - val_sparse_categorical_accuracy: 0.7400\n",
      "Epoch 29/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1407 - sparse_categorical_accuracy: 0.9643 - val_loss: 0.5662 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 30/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1060 - sparse_categorical_accuracy: 0.9745 - val_loss: 0.6994 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 31/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1164 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.6303 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 32/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1107 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.7805 - val_sparse_categorical_accuracy: 0.7200\n",
      "Epoch 33/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1338 - sparse_categorical_accuracy: 0.9643 - val_loss: 0.4215 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 34/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1087 - sparse_categorical_accuracy: 0.9643 - val_loss: 0.3684 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 35/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1073 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.3542 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 36/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1239 - sparse_categorical_accuracy: 0.9541 - val_loss: 0.3926 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 37/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1348 - sparse_categorical_accuracy: 0.9541 - val_loss: 0.4123 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 38/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1342 - sparse_categorical_accuracy: 0.9490 - val_loss: 0.4159 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 39/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1339 - sparse_categorical_accuracy: 0.9490 - val_loss: 0.5904 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 40/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0983 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.6372 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 41/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1033 - sparse_categorical_accuracy: 0.9745 - val_loss: 0.5787 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 42/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1274 - sparse_categorical_accuracy: 0.9592 - val_loss: 0.5273 - val_sparse_categorical_accuracy: 0.7400\n",
      "Epoch 43/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0765 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.4770 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 44/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0945 - sparse_categorical_accuracy: 0.9694 - val_loss: 0.4566 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 45/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1255 - sparse_categorical_accuracy: 0.9592 - val_loss: 0.4466 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 46/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0941 - sparse_categorical_accuracy: 0.9694 - val_loss: 0.3764 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 47/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0941 - sparse_categorical_accuracy: 0.9694 - val_loss: 0.3421 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 48/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1362 - sparse_categorical_accuracy: 0.9592 - val_loss: 0.3304 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 49/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0851 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.3657 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 50/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0811 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.4520 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 51/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1073 - sparse_categorical_accuracy: 0.9592 - val_loss: 0.4302 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 52/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0862 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.3844 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 53/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1011 - sparse_categorical_accuracy: 0.9745 - val_loss: 0.3647 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 54/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0816 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.3360 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 55/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0830 - sparse_categorical_accuracy: 0.9745 - val_loss: 0.3121 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 56/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0814 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.2910 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 57/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0858 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.3444 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 58/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0933 - sparse_categorical_accuracy: 0.9694 - val_loss: 0.3339 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 00058: early stopping\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"cut_from_mid_conv\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001 \n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "history = model.fit(\n",
    "    x_mid_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "190c9649-522b-43e4-9d34-2fc9f2455bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 224ms/step - loss: 0.9744 - sparse_categorical_accuracy: 0.7097\n",
      "Test accuracy 0.7096773982048035\n",
      "Test loss 0.9743765592575073\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"cut_from_mid_conv\")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_mid_test, y_test)\n",
    "\n",
    "print(\"Test accuracy\", test_acc)\n",
    "print(\"Test loss\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "806380a8-b6cf-44d3-a42a-c12e07d0b4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cut from the start\n",
    "df_start_x = df_x.applymap(lambda x: x[0:min_length])\n",
    "\n",
    "# Verify the length - number of records not meeting the length\n",
    "df_start_x[df_start_x['mfcc_0'].str.len() != min_length].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d88e6eb8-f9a0-4838-8f96-df844a5d9c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 8512, 39)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the data into (nobs, each time series feature, n feature)\n",
    "l = []\n",
    "for col in df_start_x.columns:\n",
    "    t = df_start_x.loc[:, col].to_numpy()\n",
    "    l.append(np.concatenate(t).reshape(df_start_x.shape[0], len(df_start_x.iloc[0, 0])))\n",
    "    \n",
    "np_start_x = np.dstack(l)\n",
    "np_start_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2915f0a-ce17-4a8d-a8e2-bd107259f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train/test\n",
    "x_start_train, x_start_test, y_train, y_test = train_test_split(np_start_x, df_y, test_size=0.2, random_state=363)\n",
    "\n",
    "# random sort the data for validation\n",
    "idx = np.random.permutation(len(x_start_train))\n",
    "x_start_train = x_start_train[idx]\n",
    "y_train = y_train[idx]\n",
    "\n",
    "# define number of labels\n",
    "num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "3a2c40fe-d822-47bb-b3f7-c6d5dd1cb474",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "7/7 [==============================] - 11s 1s/step - loss: 0.2537 - sparse_categorical_accuracy: 0.9235 - val_loss: 0.7653 - val_sparse_categorical_accuracy: 0.7600\n",
      "INFO:tensorflow:Assets written to: cut_from_start_conv\\assets\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 11s 1s/step - loss: 0.2327 - sparse_categorical_accuracy: 0.8980 - val_loss: 0.6296 - val_sparse_categorical_accuracy: 0.7800\n",
      "INFO:tensorflow:Assets written to: cut_from_start_conv\\assets\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1887 - sparse_categorical_accuracy: 0.9439 - val_loss: 0.8792 - val_sparse_categorical_accuracy: 0.7200\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1774 - sparse_categorical_accuracy: 0.9337 - val_loss: 0.7529 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1628 - sparse_categorical_accuracy: 0.9541 - val_loss: 0.4854 - val_sparse_categorical_accuracy: 0.8200\n",
      "INFO:tensorflow:Assets written to: cut_from_start_conv\\assets\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.1863 - sparse_categorical_accuracy: 0.9388 - val_loss: 0.4924 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1868 - sparse_categorical_accuracy: 0.9286 - val_loss: 0.4465 - val_sparse_categorical_accuracy: 0.8200\n",
      "INFO:tensorflow:Assets written to: cut_from_start_conv\\assets\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 11s 1s/step - loss: 0.2398 - sparse_categorical_accuracy: 0.9031 - val_loss: 0.8396 - val_sparse_categorical_accuracy: 0.6600\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1973 - sparse_categorical_accuracy: 0.9235 - val_loss: 0.6040 - val_sparse_categorical_accuracy: 0.7200\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.2061 - sparse_categorical_accuracy: 0.9082 - val_loss: 0.5573 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1959 - sparse_categorical_accuracy: 0.9184 - val_loss: 0.6229 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 12/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1825 - sparse_categorical_accuracy: 0.9286 - val_loss: 0.3743 - val_sparse_categorical_accuracy: 0.9000\n",
      "INFO:tensorflow:Assets written to: cut_from_start_conv\\assets\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 11s 1s/step - loss: 0.1721 - sparse_categorical_accuracy: 0.9337 - val_loss: 0.6765 - val_sparse_categorical_accuracy: 0.7400\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1707 - sparse_categorical_accuracy: 0.9490 - val_loss: 0.6431 - val_sparse_categorical_accuracy: 0.7200\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1620 - sparse_categorical_accuracy: 0.9337 - val_loss: 0.4437 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1516 - sparse_categorical_accuracy: 0.9592 - val_loss: 0.4387 - val_sparse_categorical_accuracy: 0.7400\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1616 - sparse_categorical_accuracy: 0.9592 - val_loss: 0.5217 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1843 - sparse_categorical_accuracy: 0.9439 - val_loss: 0.4213 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1899 - sparse_categorical_accuracy: 0.9286 - val_loss: 0.7586 - val_sparse_categorical_accuracy: 0.7400\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1641 - sparse_categorical_accuracy: 0.9337 - val_loss: 1.0276 - val_sparse_categorical_accuracy: 0.6800\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1434 - sparse_categorical_accuracy: 0.9541 - val_loss: 1.2916 - val_sparse_categorical_accuracy: 0.6200\n",
      "Epoch 22/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1760 - sparse_categorical_accuracy: 0.9184 - val_loss: 0.7556 - val_sparse_categorical_accuracy: 0.7000\n",
      "Epoch 23/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1486 - sparse_categorical_accuracy: 0.9694 - val_loss: 0.6030 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 24/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1428 - sparse_categorical_accuracy: 0.9592 - val_loss: 0.7080 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 25/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1268 - sparse_categorical_accuracy: 0.9694 - val_loss: 0.8570 - val_sparse_categorical_accuracy: 0.6600\n",
      "Epoch 26/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1397 - sparse_categorical_accuracy: 0.9490 - val_loss: 0.8435 - val_sparse_categorical_accuracy: 0.6200\n",
      "Epoch 27/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1841 - sparse_categorical_accuracy: 0.9235 - val_loss: 0.5825 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 28/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1410 - sparse_categorical_accuracy: 0.9592 - val_loss: 0.5359 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 29/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1098 - sparse_categorical_accuracy: 0.9694 - val_loss: 0.8147 - val_sparse_categorical_accuracy: 0.7000\n",
      "Epoch 30/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1281 - sparse_categorical_accuracy: 0.9592 - val_loss: 0.6518 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 31/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1488 - sparse_categorical_accuracy: 0.9388 - val_loss: 0.4488 - val_sparse_categorical_accuracy: 0.8800\n",
      "Epoch 32/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1242 - sparse_categorical_accuracy: 0.9490 - val_loss: 0.5836 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 33/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1305 - sparse_categorical_accuracy: 0.9643 - val_loss: 0.4345 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 34/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1344 - sparse_categorical_accuracy: 0.9592 - val_loss: 0.3658 - val_sparse_categorical_accuracy: 0.8400\n",
      "INFO:tensorflow:Assets written to: cut_from_start_conv\\assets\n",
      "Epoch 35/200\n",
      "7/7 [==============================] - 11s 2s/step - loss: 0.1073 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.3860 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 36/200\n",
      "7/7 [==============================] - 11s 1s/step - loss: 0.1017 - sparse_categorical_accuracy: 0.9694 - val_loss: 0.5218 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 37/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1010 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.5562 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 38/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1682 - sparse_categorical_accuracy: 0.9439 - val_loss: 0.6544 - val_sparse_categorical_accuracy: 0.7400\n",
      "Epoch 39/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.0920 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.8263 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 40/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1338 - sparse_categorical_accuracy: 0.9643 - val_loss: 0.9579 - val_sparse_categorical_accuracy: 0.7000\n",
      "Epoch 41/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1040 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.7942 - val_sparse_categorical_accuracy: 0.7400\n",
      "Epoch 42/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1519 - sparse_categorical_accuracy: 0.9592 - val_loss: 0.8292 - val_sparse_categorical_accuracy: 0.7000\n",
      "Epoch 43/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1191 - sparse_categorical_accuracy: 0.9694 - val_loss: 0.8029 - val_sparse_categorical_accuracy: 0.7000\n",
      "Epoch 44/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0969 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.4079 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 45/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.0973 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.3879 - val_sparse_categorical_accuracy: 0.8600\n",
      "Epoch 46/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.0975 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.6969 - val_sparse_categorical_accuracy: 0.7000\n",
      "Epoch 47/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1051 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.4952 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 48/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1043 - sparse_categorical_accuracy: 0.9745 - val_loss: 0.4047 - val_sparse_categorical_accuracy: 0.8600\n",
      "Epoch 49/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0931 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.6848 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 50/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1377 - sparse_categorical_accuracy: 0.9694 - val_loss: 0.4720 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 51/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0819 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.7167 - val_sparse_categorical_accuracy: 0.7200\n",
      "Epoch 52/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1272 - sparse_categorical_accuracy: 0.9592 - val_loss: 0.6242 - val_sparse_categorical_accuracy: 0.7200\n",
      "Epoch 53/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1045 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.4493 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 54/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1060 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.4377 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 55/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.0888 - sparse_categorical_accuracy: 0.9745 - val_loss: 0.3939 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 56/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0923 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.3669 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 57/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0749 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.3621 - val_sparse_categorical_accuracy: 0.8000\n",
      "INFO:tensorflow:Assets written to: cut_from_start_conv\\assets\n",
      "Epoch 58/200\n",
      "7/7 [==============================] - 11s 2s/step - loss: 0.0862 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.3729 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 59/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.0751 - sparse_categorical_accuracy: 0.9949 - val_loss: 0.3835 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 60/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.0753 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.3726 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 61/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1158 - sparse_categorical_accuracy: 0.9643 - val_loss: 0.3801 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 62/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.0932 - sparse_categorical_accuracy: 0.9745 - val_loss: 0.4430 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 63/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1014 - sparse_categorical_accuracy: 0.9745 - val_loss: 0.4863 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 64/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.0949 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.3866 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 65/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0918 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.3497 - val_sparse_categorical_accuracy: 0.8400\n",
      "INFO:tensorflow:Assets written to: cut_from_start_conv\\assets\n",
      "Epoch 66/200\n",
      "7/7 [==============================] - 11s 1s/step - loss: 0.0821 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.3262 - val_sparse_categorical_accuracy: 0.8400\n",
      "INFO:tensorflow:Assets written to: cut_from_start_conv\\assets\n",
      "Epoch 67/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.0854 - sparse_categorical_accuracy: 0.9745 - val_loss: 0.3646 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 68/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.0952 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.4366 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 69/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0720 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.4015 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 70/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1044 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.3403 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 71/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0872 - sparse_categorical_accuracy: 0.9694 - val_loss: 0.3743 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 72/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0919 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.5213 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 73/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1075 - sparse_categorical_accuracy: 0.9592 - val_loss: 0.5787 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 74/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.1053 - sparse_categorical_accuracy: 0.9643 - val_loss: 0.4381 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 75/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0746 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.3832 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 76/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0770 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.4315 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 77/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1093 - sparse_categorical_accuracy: 0.9745 - val_loss: 0.4623 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 78/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0870 - sparse_categorical_accuracy: 0.9745 - val_loss: 0.3998 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 79/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0775 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.4363 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 80/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.0636 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.5342 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 81/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.0690 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.8889 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 82/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0953 - sparse_categorical_accuracy: 0.9745 - val_loss: 1.0225 - val_sparse_categorical_accuracy: 0.7200\n",
      "Epoch 83/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0810 - sparse_categorical_accuracy: 0.9796 - val_loss: 1.0103 - val_sparse_categorical_accuracy: 0.7200\n",
      "Epoch 84/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0785 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.8388 - val_sparse_categorical_accuracy: 0.7200\n",
      "Epoch 85/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0869 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.7145 - val_sparse_categorical_accuracy: 0.7200\n",
      "Epoch 86/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0749 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.4917 - val_sparse_categorical_accuracy: 0.8600\n",
      "Epoch 87/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0877 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.4820 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 88/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0848 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.6047 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 89/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0671 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.7811 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 90/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0960 - sparse_categorical_accuracy: 0.9694 - val_loss: 0.8326 - val_sparse_categorical_accuracy: 0.7200\n",
      "Epoch 91/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0634 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.6840 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 92/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0926 - sparse_categorical_accuracy: 0.9745 - val_loss: 0.6467 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 93/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0698 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.7195 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 94/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0767 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.7426 - val_sparse_categorical_accuracy: 0.7600\n",
      "Epoch 95/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0583 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.7117 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 96/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0788 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.6494 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 97/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0751 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.5968 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 98/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0765 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.5859 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 99/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0848 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.5687 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 100/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0804 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.5245 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 101/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0752 - sparse_categorical_accuracy: 0.9745 - val_loss: 0.4503 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 102/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0707 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.4459 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 103/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0806 - sparse_categorical_accuracy: 0.9745 - val_loss: 0.4396 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 104/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0805 - sparse_categorical_accuracy: 0.9745 - val_loss: 0.4401 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 105/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.0665 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.4633 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 106/200\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.0699 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.4549 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 107/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0790 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.4239 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 108/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0673 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.4071 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 109/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0579 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.3882 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 110/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0597 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.3809 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 111/200\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0677 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.3942 - val_sparse_categorical_accuracy: 0.8600\n",
      "Epoch 112/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0678 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.4148 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 113/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0730 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.4338 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 114/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0609 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.4611 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 115/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0714 - sparse_categorical_accuracy: 0.9949 - val_loss: 0.5055 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 116/200\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0666 - sparse_categorical_accuracy: 0.9949 - val_loss: 0.5150 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 00116: early stopping\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"cut_from_start_conv\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "history = model.fit(\n",
    "    x_start_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b00527eb-315b-4bb2-ac46-991dc50336b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 180ms/step - loss: 0.8594 - sparse_categorical_accuracy: 0.8065\n",
      "Test accuracy 0.8064516186714172\n",
      "Test loss 0.8593730330467224\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"cut_from_start_conv\")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_start_test, y_test)\n",
    "\n",
    "print(\"Test accuracy\", test_acc)\n",
    "print(\"Test loss\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c2c19-32ef-41ea-aaa6-8dc8679c1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the start is a much superior choice (10% accurarcy improvement)\n",
    "# try with transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b6dc6c6-1679-4837-88e5-c0167a8c4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cf5987d-62f7-4ea2-9c36-3deefeb911eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0163d15-1819-4409-9270-c9238a6ab783",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[32,2,8512,8512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node model_5/multi_head_attention_16/einsum/Einsum (defined at <ipython-input-19-49ae9f45d730>:27) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_26773]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-49ae9f45d730>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m ]\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m model.fit(\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mx_start_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MusicLabel\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MusicLabel\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MusicLabel\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MusicLabel\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MusicLabel\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MusicLabel\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MusicLabel\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[32,2,8512,8512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node model_5/multi_head_attention_16/einsum/Einsum (defined at <ipython-input-19-49ae9f45d730>:27) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_26773]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "input_shape = x_start_train.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=16, #256\n",
    "    num_heads=2, #4\n",
    "    ff_dim=2, #4\n",
    "    num_transformer_blocks=2, #4\n",
    "    mlp_units=[8], #128\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"cut_from_start_trans\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    x_start_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=32, #64\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ea21e-935d-4488-bf31-e43082cd64e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOM w/ local machine, and I was happy w/ 80% accuracy for the moment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
